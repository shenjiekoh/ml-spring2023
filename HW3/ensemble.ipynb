{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally, We don't need augmentations in testing and validation.\n",
    "# All we need here is to resize the PIL image and transform it into Tensor.\n",
    "test_tfm1 = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation((-90,90),interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=False)\n",
    "])\n",
    "\n",
    "test_tfm2 = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "\n",
    "    def __init__(self,path,tfm=test_tfm,files = None):\n",
    "        super(FoodDataset).__init__()\n",
    "        self.path = path\n",
    "        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        if files != None:\n",
    "            self.files = files\n",
    "            \n",
    "        self.transform = tfm\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.transform(im)\n",
    "        \n",
    "        try:\n",
    "            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1 # test has no label\n",
    "            \n",
    "        return im,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize a model, and put it on the device specified.\n",
    "GoogLeNet = models.googlenet(weights=False)\n",
    "GoogLeNet.fc = nn.Linear(GoogLeNet.fc.in_features, 11)\n",
    "model1 = GoogLeNet.to(device)\n",
    "ResNet50 = models.resnet50(weights=False, num_classes=11)\n",
    "model2 = ResNet50.to(device)\n",
    "VGG16 = models.vgg16(weights=False, num_classes=11)\n",
    "model3 = VGG16.to(device)\n",
    "VGG13 = models.vgg13(weights=False, num_classes=11)\n",
    "model4 = VGG13.to(device)\n",
    "EfficientNet = models.efficientnet_b2(weights=False, num_classes=11)\n",
    "EfficientNet.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=1408, out_features=11, bias=True)\n",
    ")\n",
    "model5 = EfficientNet.to(device)\n",
    "\n",
    "# The number of batch size.\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set1 = FoodDataset(\"./test\", tfm=test_tfm1)\n",
    "test_loader1 = DataLoader(test_set1, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_set2 = FoodDataset(\"./test\", tfm=test_tfm2)\n",
    "test_loader2 = DataLoader(test_set2, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_state_dict(torch.load(\"./googlenet_best.ckpt\"))\n",
    "model2.load_state_dict(torch.load(\"./resnet50_best.ckpt\"))\n",
    "model3.load_state_dict(torch.load(\"./vgg16_best.ckpt\"))\n",
    "model4.load_state_dict(torch.load(\"./vgg13_best.ckpt\"))\n",
    "model5.load_state_dict(torch.load(\"./efficientnet_best.ckpt\"))\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "model5.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "labels1 = []\n",
    "labels2 = []\n",
    "labels3 = []\n",
    "labels4 = []\n",
    "labels5 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in tqdm(test_loader1):\n",
    "        pred1 = model1(data.to(device))\n",
    "        pred2 = model2(data.to(device))\n",
    "        pred3 = model3(data.to(device))\n",
    "        pred4 = model4(data.to(device))\n",
    "        label1 = np.argmax(pred1.cpu().data.numpy(), axis=1).tolist()\n",
    "        label2 = np.argmax(pred2.cpu().data.numpy(), axis=1).tolist()\n",
    "        label3 = np.argmax(pred3.cpu().data.numpy(), axis=1).tolist()\n",
    "        label4 = np.argmax(pred4.cpu().data.numpy(), axis=1).tolist()\n",
    "        labels1 += label1\n",
    "        labels2 += label2\n",
    "        labels3 += label3\n",
    "        labels4 += label4\n",
    "\n",
    "    for data, _ in tqdm(test_loader2):\n",
    "        pred5 = model5(data.to(device))\n",
    "        label5 = np.argmax(pred5.cpu().data.numpy(), axis=1).tolist()\n",
    "        labels5 += label5\n",
    "    \n",
    "labels_stack = np.row_stack((labels1, labels2, labels3, labels4, labels5))\n",
    "prediction, _ = mode(labels_stack, axis=0, keepdims=True)\n",
    "prediction = prediction.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test csv\n",
    "def pad4(i):\n",
    "    return \"0\"*(4-len(str(i)))+str(i)\n",
    "df = pd.DataFrame()\n",
    "df[\"Id\"] = [pad4(i) for i in range(len(test_set1))]\n",
    "df[\"Category\"] = prediction\n",
    "df.to_csv(\"submission.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
